{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "from neptune.integrations.lightgbm import NeptuneCallback as LGBCallback\n",
    "from neptune.integrations.xgboost import NeptuneCallback as XGBCallback\n",
    "\n",
    "from src.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quant\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import early_stopping, log_evaluation\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPTUNE_API_TOKEN_OPTUNA = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZWU5ZWRjOC02NTMxLTQ2ZGYtYmYyMS00MWVjZjQxODFmZmQifQ=='\n",
    "NEPTUNE_API_TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzYmUwOTc0Ni1jYmM2LTQ1NzEtOTBiOS05MjYzOGIwYTBiZDYifQ=='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True\n",
    "OPTUNA = False\n",
    "MODEL_NAME = \"lightgbm\"\n",
    "LOGGING_NOTE = \"running\"\n",
    "TRAIN_NAME = \"train_features.csv\"\n",
    "TEST_NAME = \"test_features.csv\"\n",
    "OPTUNA_CV = \"StratifiedKFold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quant\\miniconda3\\Lib\\site-packages\\neptune\\common\\warnings.py:71: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/edwardbk29/KHDL/e/KHDL-2\n"
     ]
    }
   ],
   "source": [
    "PROJECT = \"edwardbk29/KHDL\"\n",
    "PROJECT_OPTUNA = \"quan-tran-tu/KHDL-optuna\"\n",
    "SOURCE = \"13.ipynb\"\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=PROJECT,\n",
    "    source_files=[SOURCE],\n",
    "    api_token=NEPTUNE_API_TOKEN,\n",
    ")\n",
    "\n",
    "if MODEL_NAME == \"xgboost\":\n",
    "    neptune_callback = LGBCallback(run=run)\n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "    neptune_callback = XGBCallback(run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"note\"] = LOGGING_NOTE\n",
    "run[\"sys/tags\"].add([MODEL_NAME,])\n",
    "run['dataset/train'] = TRAIN_NAME\n",
    "run['dataset/test'] = TEST_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"model/parameters/GPU\"] = GPU\n",
    "run[\"model/parameters/OPTUNA\"] = OPTUNA\n",
    "\n",
    "if OPTUNA:\n",
    "    run[\"model/optuna/optuna_cv\"] = OPTUNA_CV\n",
    "    run[\"model/optuna/optuna_folds\"] = OPTUNA_FOLDS = 5\n",
    "    run[\"model/optuna/optuna_trials\"] = OPTUNA_TRIALS = 150\n",
    "\n",
    "run[\"model/parameters/k_folds\"] = K_FOLDS = 5\n",
    "run[\"model/parameters/seed\"] = SEED = 13\n",
    "run[\"model/parameters/num_boost_round\"] = NUM_BOOST_ROUND = 2000 #xgb param\n",
    "run[\"model/parameters/enable_categorical\"] = ENABLE_CATEGORICAL = False\n",
    "run[\"model/parameters/early_stopping\"] = EARLY_STOPPING = 200 \n",
    "\n",
    "XGB_VERBOSITY = 0 #xgb param\n",
    "LGB_VERBOSITY = -1 #lgb param\n",
    "VERBOSE_EVAL = False #lgb param\n",
    "LOG_EVALUATION = 10000 #lgb display parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"xgboost\":\n",
    "\n",
    "    BASE_MODEL = xgb\n",
    "    \n",
    "    STATIC_PARAMS = {\n",
    "                    'seed': SEED,\n",
    "                    'eval_metric': 'auc',\n",
    "                    \"objective\": \"binary:logistic\",\n",
    "                    'verbosity': XGB_VERBOSITY,\n",
    "                    }\n",
    "\n",
    "    GPU_PARAMS = {\n",
    "                 'tree_method': 'gpu_hist',\n",
    "                 'predictor': 'gpu_predictor',\n",
    "                 }\n",
    "    \n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "    \n",
    "    BASE_MODEL = lgb\n",
    "    \n",
    "    STATIC_PARAMS = {\n",
    "                    'seed': SEED,\n",
    "                    'verbosity': LGB_VERBOSITY,           \n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'auc', \n",
    "                    }\n",
    "\n",
    "    GPU_PARAMS = {\n",
    "                'device': 'gpu',\n",
    "                'gpu_platform_id': 0,\n",
    "                'gpu_device_id': 0,\n",
    "                 }\n",
    "\n",
    "\n",
    "if GPU:\n",
    "    STATIC_PARAMS = STATIC_PARAMS | GPU_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_NAME)\n",
    "test = pd.read_csv(TEST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_COLUMNS = ['SEASON', 'HOME_TEAM_ID', 'VISITOR_TEAM_ID' ]\n",
    "\n",
    "train = encode_categoricals(train, CATEGORY_COLUMNS, MODEL_NAME, ENABLE_CATEGORICAL)\n",
    "test = encode_categoricals(test, CATEGORY_COLUMNS, MODEL_NAME, ENABLE_CATEGORICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLUMNS = ['TARGET', 'GAME_DATE_EST', 'GAME_ID', ]\n",
    "target = train['TARGET']\n",
    "test_target = test['TARGET']\n",
    "test_target_original = test['TARGET'] #save for later probability calibration\n",
    "\n",
    "all_columns = remove_non_rolling(train)\n",
    "\n",
    "use_columns = [item for item in all_columns if item not in DROP_COLUMNS]\n",
    "\n",
    "\n",
    "train = train[use_columns]\n",
    "test = test[use_columns]\n",
    "test_original = test.copy() #save for later probability calibration\n",
    "\n",
    "run[\"model/features\"].log(use_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna():\n",
    "    \n",
    "    #log separate Neptune run for optuna hyperameter tuning\n",
    "    run2 = neptune.init_run(\n",
    "                    project=PROJECT_OPTUNA,\n",
    "                    source_files=[SOURCE,],\n",
    "                    api_token=NEPTUNE_API_TOKEN_OPTUNA,\n",
    "                    )\n",
    "    run2[\"options/optuna_cv\"] = OPTUNA_CV \n",
    "    run2[\"options/optuna_folds\"] = OPTUNA_FOLDS \n",
    "    run2[\"options/optuna_trials\"] = OPTUNA_TRIALS \n",
    "    run2[\"options/GPU\"] = GPU\n",
    "    run2[\"options/enable_categorical\"] = ENABLE_CATEGORICAL\n",
    "    run2[\"features\"].log(use_columns)\n",
    "    run2[\"sys/tags\"].add([MODEL_NAME,])\n",
    "    \n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        func = lambda trial: XGB_objective(trial, train, target, STATIC_PARAMS, ENABLE_CATEGORICAL, NUM_BOOST_ROUND, OPTUNA_CV, OPTUNA_FOLDS, SEED)\n",
    "    if MODEL_NAME == \"lightgbm\":\n",
    "        func = lambda trial: LGB_objective(trial, train, target, CATEGORY_COLUMNS, STATIC_PARAMS, ENABLE_CATEGORICAL, NUM_BOOST_ROUND, OPTUNA_CV, OPTUNA_FOLDS, SEED, EARLY_STOPPING)         \n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(func, n_trials = OPTUNA_TRIALS,)\n",
    "\n",
    "    #optuna_utils.log_study_metadata(study, run2)\n",
    "    \n",
    "    print(\"Study Best Value:\",study.best_value)\n",
    "    print(\"Study Best Params:\",study.best_params)\n",
    "    \n",
    "    plot_optimization_history(study)\n",
    "    \n",
    "    plot_param_importances(study)\n",
    "    \n",
    "    run2[\"best_value\"] = study.best_value\n",
    "    run2[\"best_params\"] = study.best_params\n",
    "    run2[\"static_params\"] = STATIC_PARAMS\n",
    "    \n",
    "    run2.stop()\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTUNA:\n",
    "    tuned_params = run_optuna()\n",
    "else:\n",
    "    with open(('configs/' + MODEL_NAME + '.json')) as f:\n",
    "        tuned_params = json.loads(f.read())\n",
    "\n",
    "model_params= STATIC_PARAMS | tuned_params\n",
    "\n",
    "run[\"model/params\"] = model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(target, preds):\n",
    "    #for accuracy score, prediction probabilities must be convert to binary scores (Win or Lose)\n",
    "    #determine optimum threshold for converting probabilities using ROC curve\n",
    "    #generally 0.5 works for balanced data\n",
    "    #fpr = false positive rate, tpr = true positive rate\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target,preds)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_binary = (preds > optimal_threshold).astype(int)\n",
    "    \n",
    "    acc_score = accuracy_score(target, preds_binary)\n",
    "    auc_score = roc_auc_score(target, preds)\n",
    "\n",
    "    print()\n",
    "    print(\"Scores:\")\n",
    "    print()\n",
    "    print(\"Accuracy Score:\", acc_score)\n",
    "    print(\"AUC Score:\", auc_score)\n",
    "    print(\"Optimal Threshold:\", optimal_threshold)\n",
    "    \n",
    "    return preds_binary, acc_score, auc_score, optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapley(MODEL_NAME, model, data):\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        shap = model.predict(data, pred_contribs=True)\n",
    "    if MODEL_NAME == \"lightgbm\":\n",
    "        shap = model.predict(data, pred_contrib=True)\n",
    "        \n",
    "    return shap\n",
    "\n",
    "def get_shapley_interactions(MODEL_NAME, model, data):\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        shap_interactions = model.predict(data, pred_interactions=True)\n",
    "    if MODEL_NAME == \"lightgbm\": #not currently supported\n",
    "        shap_interactions = np.zeros((data.shape[0],data.shape[1]+1,data.shape[1]+1))\n",
    "    \n",
    "    return shap_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize oof arrays including Shapley values and Shapley interaction values\n",
    "train_oof = np.zeros((train.shape[0],))\n",
    "train_oof_shap = np.zeros((train.shape[0],train.shape[1]+1))\n",
    "# train_oof_shap_interact = np.zeros((train.shape[0],train.shape[1]+1,train.shape[1]+1))\n",
    "\n",
    "   \n",
    "# K-fold cross validation\n",
    "if OPTUNA_CV == \"StratifiedKFold\": \n",
    "    kf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "elif OPTUNA_CV == \"TimeSeriesSplit\":\n",
    "    kf = TimeSeriesSplit(n_splits=K_FOLDS)\n",
    "\n",
    "\n",
    "for f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n",
    "    \n",
    "    train_df, val_df = train.iloc[train_ind], train.iloc[val_ind]\n",
    "    train_target, val_target = target[train_ind], target[val_ind]\n",
    "\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        train_dmatrix = xgb.DMatrix(train_df, label=train_target,enable_categorical=ENABLE_CATEGORICAL)\n",
    "        val_dmatrix = xgb.DMatrix(val_df, label=val_target,enable_categorical=ENABLE_CATEGORICAL)\n",
    "        val_data = val_dmatrix\n",
    "       \n",
    "        model =  xgb.train(model_params, \n",
    "                           train_dmatrix, \n",
    "                           num_boost_round = NUM_BOOST_ROUND,\n",
    "                          callbacks=[neptune_callback],\n",
    "                          )\n",
    "    \n",
    "    if MODEL_NAME == \"lightgbm\":\n",
    "        train_lgbdataset = lgb.Dataset(train_df, label=train_target, categorical_feature=CATEGORY_COLUMNS)\n",
    "        val_lgbdataset = lgb.Dataset(val_df, label=val_target, reference = train_lgbdataset, categorical_feature=CATEGORY_COLUMNS)\n",
    "        val_data = val_df\n",
    "        \n",
    "        model =  lgb.train(model_params, \n",
    "                       train_lgbdataset,\n",
    "                       valid_sets=val_lgbdataset,\n",
    "                       #num_boost_round = NUM_BOOST_ROUND,\n",
    "                       callbacks=[log_evaluation(LOG_EVALUATION), neptune_callback],\n",
    "                      )\n",
    "    \n",
    "    temp_oof = model.predict(val_data)\n",
    "    temp_oof_shap = get_shapley(MODEL_NAME, model, val_data)\n",
    "    temp_oof_shap_interact = get_shapley_interactions(MODEL_NAME, model, val_data)\n",
    "\n",
    "    train_oof[val_ind] = temp_oof\n",
    "\n",
    "    train_oof_shap[val_ind, :] = temp_oof_shap\n",
    "    # train_oof_shap_interact[val_ind, :,:] = temp_oof_shap_interact\n",
    "    \n",
    "    temp_oof_binary, acc_score, auc_score, optimal_threshold = get_scores(val_target, temp_oof)\n",
    "\n",
    "# Out-of-Fold composite for train data\n",
    "\n",
    "train_oof_binary, acc_score, auc_score, optimal_threshold = get_scores(target,train_oof)\n",
    "\n",
    "run[\"train/accuracy\"] = acc_score \n",
    "run[\"train/AUC\"] = auc_score \n",
    "run[\"train/optimal_threshold\"] = optimal_threshold\n",
    "\n",
    "df = {'Label': 'Train', 'Accuracy': acc_score, 'AUC': auc_score, 'Threshold':optimal_threshold}\n",
    "results = results.append(df, ignore_index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"xgboost\": \n",
    "    model = XGBClassifier(n_estimators=NUM_BOOST_ROUND, **model_params) \n",
    "if MODEL_NAME == \"lightgbm\": \n",
    "    model = LGBMClassifier(verbose_eval=False, **model_params)\n",
    "# we then set up CalibratedClassifierCV using Isotonic and Sigmoid Regression \n",
    "model_isotonic = CalibratedClassifierCV(model, cv=5, method=\"isotonic\") \n",
    "model_sigmoid = CalibratedClassifierCV(model, cv=5, method=\"sigmoid\") \n",
    "clf_list = [ (model, \"Base Model\"), (model_isotonic, \"Model + Isotonic\"), (model_sigmoid, \"Model + Sigmoid\"), ] \n",
    "y_train = target \n",
    "y_test = test_target_original \n",
    "X_train = train[use_columns] \n",
    "X_test = test_original \n",
    "plot_calibration_curve(clf_list, X_train, y_train, X_test, y_test, n_bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores, clf_list = calculate_classification_metrics(clf_list, X_train, y_train, X_test, y_test) \n",
    "df_scores = df_scores.reset_index() \n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [classifier for (classifier, name) in clf_list if name == best_calibrated_model][0] \n",
    "joblib.dump(model, MODELS_PATH / 'model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
